TimeLayers
time embeddingなどのたtimeレイヤは順伝播時にT個のenbeddingレイヤを用意し、各Embeddingレイヤが書く時刻のデータを処理する


RNNレイヤにおける勾配消失と勾配爆発
通常のRNNレイヤでは、時間を遡るに従って、勾配が非常に小さくなってほとんど何の情報ももたなくなってしまう勾配消失や、逆に勾配がオーバーフローして学習が行えなくなる勾配爆発がおこってしまう

勾配爆発の対策としては、勾配クリッピングを行い勾配がしきい値を超えた場合に修正する
では、勾配消失の対策は？
対策としてゲート付きRNNがあり、その代表格がGRUとLSTM

ここではLSTMレイヤの組み立てに焦点を当てる
LSTMではcRNNにcという新たな経路が追加される。　c: 記憶セル。記憶部に相当
ここで、入力 xtと前レイヤの記憶セル、隠れ状態であるct-1, ht-1を用いて、次の状態であるct, htを求める。
ここで、ゲートではどの程度情報を引き継ぐかを調節している→0 ~ 100%を決めている→sigmoidを用いる

RNNに追加される点
-----------------------
outputゲートo:現在のセル状態ctを、どの程度現在の状態htに反映するかを、ht-1, xtをもとに決定する
forgetゲートf:前のセル状態ct-1を、どの程度次のセル状態ctに保持するかを、ht-1, xtをもとに決定する
新しく記憶セルに追加する情報g:前の状態ht-1と、入力xtから、新しい情報をforgetゲート通過後のセルct-1に追加する。これはゲートではないため、tanhを用いる
inputゲートi：セルに追加する情報gが、新たに追加する除法としてどれだけ価値があるかをht-1, xtをもとに決定する。
------------------------

記憶セルの逆伝播を見てみると＋と＊のみを通過する。また、＊は行列席ではなく、要素ごとの席なので、勾配消失が起こりにくい。

これらの計算をアフィン変換を用いてひとまとまりに計算できるようにしている

*同一LSTM層の異なる時間ステップ→同じ重み。　異なるLSTM層→重みは異なる